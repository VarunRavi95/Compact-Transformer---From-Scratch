loading datasets...
Train dataset loaded
Liger kernel not available, using standard cross entropy
Model on device cpu is ready
Device is:  cpu
Device is:  cpu
Loaders ready both
C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py:330: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=torch.cuda.is_available())
Training info: 1 epochs, 313264 batches per epoch, 313264 total batches
  0%|                                                                                                     | 0/10000 [00:00<?, ?it/s]C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_dynamo\variables\functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `nt._path_isdir.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
Starting with val evaluation...
If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
  torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))
  0%|                                                                                                     | 0/10000 [01:04<?, ?it/s]
Traceback (most recent call last):
  File "C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py", line 475, in <module>
    train()
    ~~~~~^^
  File "C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py", line 343, in train
    losses = estimate_loss(val_loader, val_data_iterator, model_args.device, model, model_args)
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\utils\_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py", line 262, in estimate_loss
    loss = model(idx, targets_idx, targets, src_mask, tgt_mask)
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_dynamo\eval_frame.py", line 414, in __call__
    return super().__call__(*args, **kwargs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\nn\modules\module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\nn\modules\module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_dynamo\eval_frame.py", line 845, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\compile_fx.py", line 990, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
        e.__traceback__
    ) from None
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\compile_fx.py", line 974, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
        gm, example_inputs, inputs_to_check, **graph_kwargs
    )
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\compile_fx.py", line 1695, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\compile_fx.py", line 1505, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\graph.py", line 2319, in compile_to_module
    return self._compile_to_module()
           ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\graph.py", line 2325, in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\graph.py", line 2264, in codegen
    self.scheduler.codegen()
    ~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\scheduler.py", line 5197, in codegen
    self._codegen_partitions()
    ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\scheduler.py", line 5337, in _codegen_partitions
    self._codegen(partition)
    ~~~~~~~~~~~~~^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\scheduler.py", line 5435, in _codegen
    self.get_backend(device).codegen_node(node)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\codegen\cpp.py", line 5282, in codegen_node
    cpp_kernel_proxy = self.kernel_proxy_cls(kernel_group)
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\codegen\cpp.py", line 4008, in __init__
    self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()
                                              ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 497, in pick_vec_isa
    _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()
                                        ~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 484, in valid_vec_isa_list
    isa_list.extend(
    ~~~~~~~~~~~~~~~^
        isa
        ^^^
        for isa in supported_vec_isa_list
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 487, in <genexpr>
    if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa
                                                                            ^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 143, in __bool__
    return self.__bool__impl(config.cpp.vec_isa_ok)
           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 153, in __bool__impl
    return self.check_build(VecISA._avx_code)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 103, in check_build
    extra=_get_isa_dry_compile_fingerprint(self._arch_flags),
          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpu_vec_isa.py", line 29, in _get_isa_dry_compile_fingerprint
    compiler_info = get_compiler_version_info(get_cpp_compiler())
                                              ~~~~~~~~~~~~~~~~^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpp_builder.py", line 338, in get_cpp_compiler
    check_compiler_exist_windows(compiler)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\SRVarun\miniconda3\Lib\site-packages\torch\_inductor\cpp_builder.py", line 139, in check_compiler_exist_windows
    raise RuntimeError(f"Compiler: {compiler} is not found.") from exc
torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
