tokenizer_config.json: 100%|████████████████████████████████████████████████| 485/485 [00:00<00:00, 2.58MB/s]
C:\Users\SRVarun\miniconda3\Lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\SRVarun\.cache\huggingface\hub\models--ai4bharat--IndicBARTSS. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
spiece.model: 100%|█████████████████████████████████████████████████████| 1.80M/1.80M [00:00<00:00, 2.27MB/s]
added_tokens.json: 100%|█████████████████████████████████████████████████████| 239/239 [00:00<00:00, 769kB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████| 398/398 [00:00<00:00, 1.02MB/s]
config.json: 100%|██████████████████████████████████████████████████████████| 832/832 [00:00<00:00, 2.04MB/s]
Traceback (most recent call last):
  File "C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py", line 474, in <module>
    train()
    ~~~~~^^
  File "C:\Users\SRVarun\OneDrive - Rapyder Cloud Solutions Pvt Ltd\Desktop\Compact Transformer - From Scratch\trainer.py", line 288, in train
    model_args.src_vocab_size = len(tokenizer)
                                ~~~^^^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
